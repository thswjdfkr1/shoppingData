{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1lfQ10kJVS8tIDeRrigPg4lXzak1e8Mrd","authorship_tag":"ABX9TyNKAUV4p92k769sXKC+LTsk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIJKHWmMHqSN","executionInfo":{"status":"ok","timestamp":1736744960640,"user_tz":-540,"elapsed":39439,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"b370b9ea-7789-4e90-8e47-059f50692689"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.7\n","Collecting optuna\n","  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"]}],"source":["# Suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os, sys, gc, warnings, random\n","from typing import List\n","\n","import datetime\n","import dateutil.relativedelta\n","\n","# Data manipulation\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Machine learning\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GroupKFold\n","from sklearn.ensemble import RandomForestClassifier\n","\n","import lightgbm as lgb\n","import xgboost as xgb\n","!pip install catboost\n","from catboost import CatBoostClassifier, Pool\n","\n","!pip install optuna\n","import optuna\n","\n","from tqdm.notebook import trange, tqdm\n","\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","pd.options.display.max_rows = 10000\n","pd.options.display.max_columns = 1000\n","pd.options.display.max_colwidth = 1000"]},{"cell_type":"code","source":["def seed_everything(seed=0):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","SEED = 42\n","seed_everything(SEED)"],"metadata":{"id":"Ue7y_fYTHvPp","executionInfo":{"status":"ok","timestamp":1736744960641,"user_tz":-540,"elapsed":16,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["TOTAL_THRES = 300\n","\n","'''\n","    입력인자로 받는 year_month에 대해 고객 ID별로 총 구매액이\n","    구매액 임계값을 넘는지 여부의 binary label을 생성하는 함수\n","'''\n","def generate_label(\n","    df: pd.DataFrame,\n","    year_month: str,\n","    total_thres: int = TOTAL_THRES,\n","    print_log: bool = False\n","):\n","    df = df.copy()\n","\n","    # year_month에 해당하는 label 데이터 생성\n","    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n","    df.reset_index(drop=True, inplace=True)\n","\n","    # year_month 이전 월의 고객 ID 추출\n","    cust = df[df['year_month']<year_month]['customer_id'].unique()\n","    # year_month에 해당하는 데이터 선택\n","    df = df[df['year_month']==year_month]\n","\n","    # label 데이터프레임 생성\n","    label = pd.DataFrame({'customer_id':cust})\n","    label['year_month'] = year_month\n","\n","    # year_month에 해당하는 고객 ID의 구매액의 합 계산\n","    grped = df.groupby(['customer_id','year_month'], as_index=False)[['total']].sum()\n","\n","    # label 데이터프레임과 merge하고 구매액 임계값을 넘었는지 여부로 label 생성\n","    label = label.merge(grped, on=['customer_id','year_month'], how='left')\n","    label['total'].fillna(0.0, inplace=True)\n","    label['label'] = (label['total'] > total_thres).astype(int)\n","\n","    # 고객 ID로 정렬\n","    label = label.sort_values('customer_id').reset_index(drop=True)\n","    if print_log: print(f'{year_month} - final label shape: {label.shape}')\n","\n","    return label"],"metadata":{"id":"rgf4nAPsH4k6","executionInfo":{"status":"ok","timestamp":1736744960642,"user_tz":-540,"elapsed":16,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["'''\n","    평가지표를 출력하는 함수\n","'''\n","def print_score(label: List[float], pred: List[float], prob_thres: float = 0.5):\n","    print('Precision: {:.5f}'.format(precision_score(label, pred>prob_thres)))\n","    print('Recall: {:.5f}'.format(recall_score(label, pred>prob_thres)))\n","    print('F1 Score: {:.5f}'.format(f1_score(label, pred>prob_thres)))\n","    print('ROC AUC Score: {:.5f}'.format(roc_auc_score(label, pred)))"],"metadata":{"id":"lg-lYOJ1H6S7","executionInfo":{"status":"ok","timestamp":1736744960642,"user_tz":-540,"elapsed":14,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["'''\n","    입력인자로 받는 train, test 데이터에 대해 범주형 피쳐는 레이블 인코딩을\n","    진행하고 결측치에 대해서는 중위값으로 데이터 전처리를 하는 함수\n","'''\n","def feature_preprocessing(\n","    train: pd.DataFrame,\n","    test: pd.DataFrame,\n","    features: list,\n","    do_imputing=True\n","):\n","    x_tr = train.copy()\n","    x_te = test.copy()\n","\n","    # 범주형 피처 이름을 저장할 변수\n","    cate_cols = []\n","\n","    # 레이블 인코딩\n","    for f in features:\n","        if x_tr[f].dtype.name == 'object': # 데이터 타입이 object(str)이면 레이블 인코딩\n","            cate_cols.append(f)\n","            le = LabelEncoder()\n","            # train + test 데이터를 합쳐서 레이블 인코딩 함수에 fit\n","            le.fit(list(x_tr[f].values) + list(x_te[f].values))\n","\n","            # train 데이터 레이블 인코딩 변환 수행\n","            x_tr[f] = le.transform(list(x_tr[f].values))\n","\n","            # test 데이터 레이블 인코딩 변환 수행\n","            x_te[f] = le.transform(list(x_te[f].values))\n","\n","    print('categorical feature:', cate_cols)\n","\n","    if do_imputing:\n","        # 중위값으로 결측치 채우기\n","        imputer = SimpleImputer(strategy='median')\n","\n","        x_tr[features] = imputer.fit_transform(x_tr[features])\n","        x_te[features] = imputer.transform(x_te[features])\n","\n","    return x_tr, x_te"],"metadata":{"id":"-ZBrRDQpH7kD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feature_engineering1(df: pd.DataFrame, year_month: str):\n","    df = df.copy()\n","\n","    # year_month 이전 월 계산\n","    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n","    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n","    prev_ym = prev_ym.strftime('%Y-%m')\n","\n","    # train, test 데이터 선택\n","    train = df[df['order_date'] < prev_ym]\n","    test = df[df['order_date'] < year_month]\n","\n","    # train, test 레이블 데이터 생성\n","    train_label = generate_label(df, prev_ym)[['customer_id','year_month','label']]\n","    test_label = generate_label(df, year_month)[['customer_id','year_month','label']]\n","\n","    # group by aggregation 함수 선언\n","    agg_func = ['mean','max','min','sum','count','std','skew']\n","    all_train_data_list = []  # 여러 데이터프레임을 담을 리스트\n","\n","    for i, tr_ym in enumerate(train_label['year_month'].unique()):\n","        # 숫자형 컬럼만 선택\n","        numeric_cols = train.select_dtypes(include=[np.number]).columns\n","        train_numeric = train.loc[train['order_date'] < tr_ym, numeric_cols]\n","\n","        # group by aggretation 함수로 train 데이터 피처 생성\n","        train_agg = train_numeric.groupby(['customer_id']).agg(agg_func)\n","\n","        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n","        new_cols = []\n","        for col in train_agg.columns.levels[0]:\n","            for stat in train_agg.columns.levels[1]:\n","                new_cols.append(f'{col}-{stat}')\n","\n","        train_agg.columns = new_cols\n","        train_agg.reset_index(inplace=True)\n","\n","        train_agg['year_month'] = tr_ym\n","\n","        all_train_data_list.append(train_agg)  # 리스트에 데이터프레임 추가\n","\n","    # 리스트의 데이터프레임들을 하나로 합치기\n","    all_train_data = pd.concat(all_train_data_list, ignore_index=True)\n","\n","    all_train_data = train_label.merge(all_train_data, on=['customer_id', 'year_month'], how='left')\n","    features = all_train_data.drop(columns=['customer_id', 'label', 'year_month']).columns\n","\n","    # group by aggretation 함수로 test 데이터 피처 생성\n","    test_numeric = test[numeric_cols]\n","    test_agg = test_numeric.groupby(['customer_id']).agg(agg_func)\n","    test_agg.columns = new_cols\n","\n","    test_data = test_label.merge(test_agg, on=['customer_id'], how='left')\n","\n","    # train, test 데이터 전처리\n","    x_tr, x_te = feature_preprocessing(all_train_data, test_data, features)\n","\n","    print('x_tr.shape', x_tr.shape, ', x_te.shape', x_te.shape)\n","\n","    return x_tr, x_te, all_train_data['label'], features\n"],"metadata":{"id":"SgnwHuq9Nlsl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","    피처 중요도 정규화(importance_normalized) 및 누적 중요도(cumulative_importance)를 계산하고\n","    중요도 정규화 순으로 n개의 바플롯을 그리는 함수\n","'''\n","def plot_feature_importances(df, n=20, color='blue', figsize=(12,8)):\n","    # 피처 중요도 순으로 내림차순 정렬\n","    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n","\n","    # 피처 중요도 정규화 및 누적 중요도 계산\n","    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n","    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n","\n","    plt.rcParams['font.size'] = 12\n","    plt.style.use('fivethirtyeight')\n","    # 피처 중요도 순으로 n개까지 바플롯으로 그리기\n","    df.loc[:n, :].plot.barh(y='importance_normalized',\n","                            x='feature', color=color,\n","                            edgecolor='k', figsize=figsize,\n","                            legend=False)\n","\n","    plt.xlabel('Normalized Importance', size=18); plt.ylabel('');\n","    plt.title(f'Top {n} Most Important Features', size=18)\n","    plt.gca().invert_yaxis()\n","\n","    return df"],"metadata":{"id":"UjZzGZD3H-gM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/머신러닝 거의 총정리 느낌/boostcourse ai224 codes/input/train.csv\", parse_dates=[\"order_date\"])\n","print(data.shape)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"JjZcZjfFIFVO","executionInfo":{"status":"error","timestamp":1727772302396,"user_tz":-540,"elapsed":919,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"3c8ff8ee-33fa-452f-d561-a5da066ca382"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/머신러닝 거의 총정리 느낌/boostcourse ai224 codes/input/train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-52fa5182343e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/머신러닝 거의 총정리 느낌/boostcourse ai224 codes/input/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/머신러닝 거의 총정리 느낌/boostcourse ai224 codes/input/train.csv'"]}]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"3zoZHX6W6utN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.isna().sum()"],"metadata":{"id":"J1oDjn5i6v2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe(include='all')"],"metadata":{"id":"0lBfqZ2r6xEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_2011_11 = generate_label(data, '2011-11')['label']"],"metadata":{"id":"uXqTZ-HkIWoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.countplot(x='label', data=label_2011_11.to_frame());\n","label_2011_11.value_counts()"],"metadata":{"id":"0EYU57M8IaNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","    학습 데이터(train), 테스트 데이터(test)로 LightGBM 모델을\n","    학습 및 테스트하고 사용된 피처들의 중요도를 반환하는 함수\n","'''\n","def make_lgb_prediction(train, y, test, features, categorical_features='auto', model_params=None):\n","    x_train = train[features]\n","    x_test = test[features]\n","\n","    print(x_train.shape, x_test.shape)\n","\n","    # 피처 중요도를 저장할 데이터 프레임 선언\n","    fi = pd.DataFrame()\n","    fi['feature'] = features\n","\n","    # LightGBM 데이터셋 선언\n","    dtrain = lgb.Dataset(x_train, label=y)\n","\n","    # LightGBM 모델 훈련\n","    clf = lgb.train(\n","        model_params,\n","        dtrain,\n","        categorical_feature=categorical_features,\n","    )\n","\n","    # 테스트 데이터 예측\n","    test_preds = clf.predict(x_test)\n","\n","    # 피처 중요도 저장\n","    fi['importance'] = clf.feature_importance()\n","\n","    return test_preds, fi"],"metadata":{"id":"YQwxnGU-H9Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_params = {\n","    'objective': 'binary', # 이진 분류\n","    'boosting_type': 'gbdt',\n","    'metric': 'auc', # 평가 지표 설정\n","    'feature_fraction': 0.8, # 피처 샘플링 비율\n","    'bagging_fraction': 0.8, # 데이터 샘플링 비율\n","    'bagging_freq': 1,\n","    'n_estimators': 100, # 트리 개수\n","    'seed': SEED,\n","    'verbose': -1,\n","    'n_jobs': -1,\n","}"],"metadata":{"id":"9640Ci_5Ia7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test, y, features = feature_engineering1(data, '2011-11')\n","test_preds_2011_11, fi = make_lgb_prediction(train, y, test, features, model_params=model_params)\n","print_score(label_2011_11, test_preds_2011_11)"],"metadata":{"id":"GjY2aVCgJBOx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.distplot(test_preds_2011_11)\n","plt.show()"],"metadata":{"id":"AdqRkKx-JB7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test, y, features = feature_engineering1(data, '2011-12')\n","test_preds, fi = make_lgb_prediction(train, y, test, features, model_params=model_params)"],"metadata":{"id":"z2CElQ1OJDk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.distplot(test_preds)\n","plt.show()"],"metadata":{"id":"4nEMdHcjJEcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi = plot_feature_importances(fi)"],"metadata":{"id":"KKChowE2JFuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_dir = '/content/drive/MyDrive/머신러닝 거의 총정리 느낌'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","pd.DataFrame(test_preds).to_csv(os.path.join(output_dir, 'output.csv'), index=False)\n","os.makedirs(output_dir, exist_ok=True)\n","\n","submission = pd.read_csv('/content/drive/MyDrive/머신러닝 거의 총정리 느낌/boostcourse ai224 codes/input/sample_submission.csv')\n","submission['probability'] = test_preds\n","submission.to_csv(os.path.join(output_dir, 'output_lgbm.csv'), index=False)"],"metadata":{"collapsed":true,"id":"JkiC2lqAJGgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델비교"],"metadata":{"id":"O3S0SUDV8-F7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","    피처 중요도 정규화(importance_normalized) 및 누적 중요도(cumulative_importance)를 계산하고\n","    중요도 정규화 순으로 n개의 바플롯을 그리는 함수\n","'''\n","def plot_feature_importances(df, n=20, color='blue', figsize=(12,8)):\n","    # 피처 중요도 순으로 내림차순 정렬\n","    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n","\n","    # 피처 중요도 정규화 및 누적 중요도 계산\n","    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n","    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n","\n","    plt.rcParams['font.size'] = 12\n","    plt.style.use('fivethirtyeight')\n","    # 피처 중요도 순으로 n개까지 바플롯으로 그리기\n","    df.loc[:n, :].plot.barh(y='importance_normalized',\n","                            x='feature', color=color,\n","                            edgecolor='k', figsize=figsize,\n","                            legend=False)\n","\n","    plt.xlabel('Normalized Importance', size=18); plt.ylabel('');\n","    plt.title(f'Top {n} Most Important Features', size=18)\n","    plt.gca().invert_yaxis()\n","\n","    return df"],"metadata":{"id":"cN9ox2mU1UGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lgbm K fold cross validation\n","\n","'''\n","    학습 데이터(x_tr), 검증 데이터(x_val), 테스트 데이터(test)로 LightGBM 모델을\n","    학습, 교차(cross) 검증 및 테스트하고 사용된 피처들의 중요도를 반환하는 함수\n","'''\n","\n","def make_lgb_oof_prediction(train, y, test, features, categorical_features='auto', model_params=None, folds=10):\n","    x_train = train[features]\n","    x_test = test[features]\n","\n","    # 테스트 데이터 예측값을 저장할 변수\n","    test_preds = np.zeros(x_test.shape[0])\n","\n","    # Out Of Fold Validation 예측 데이터를 저장할 변수\n","    y_oof = np.zeros(x_train.shape[0])\n","\n","    # 폴드별 평균 Validation 스코어를 저장할 변수\n","    score = 0\n","\n","    # 피처 중요도를 저장할 데이터 프레임 선언\n","    fi = pd.DataFrame()\n","    fi['feature'] = features\n","\n","    # Stratified K Fold 선언\n","    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n","\n","    for fold, (tr_idx, val_idx) in enumerate(skf.split(x_train, y)):\n","        # train index, validation index로 train 데이터를 나눔\n","        x_tr, x_val = x_train.loc[tr_idx, features], x_train.loc[val_idx, features]\n","        y_tr, y_val = y[tr_idx], y[val_idx]\n","\n","        print(f'fold: {fold+1}, x_tr.shape: {x_tr.shape}, x_val.shape: {x_val.shape}')\n","\n","        # LightGBM 데이터셋 선언\n","        dtrain = lgb.Dataset(x_tr, label=y_tr,categorical_feature=categorical_features)\n","        dvalid = lgb.Dataset(x_val, label=y_val,categorical_feature=categorical_features)\n","\n","        # LightGBM 모델 훈련\n","        clf = lgb.train(\n","            params=model_params,\n","            train_set=dtrain,\n","            valid_sets=[dtrain, dvalid], # Validation 성능을 측정할 수 있도록 설정\n","            categorical_feature=categorical_features,\n","            verbose=200\n","        )\n","\n","        # Validation 데이터 예측\n","        val_preds = clf.predict(x_val)\n","\n","        # Validation index에 예측값 저장\n","        y_oof[val_idx] = val_preds\n","\n","        # 폴드별 Validation 스코어 측정\n","        print(f\"Fold {fold + 1} | AUC: {roc_auc_score(y_val, val_preds)}\")\n","        print('-'*80)\n","\n","        # score 변수에 폴드별 평균 Validation 스코어 저장\n","        score += roc_auc_score(y_val, val_preds) / folds\n","\n","        # 테스트 데이터 예측하고 평균해서 저장\n","        test_preds += clf.predict(x_test) / folds\n","\n","        # 폴드별 피처 중요도 저장\n","        fi[f'fold_{fold+1}'] = clf.feature_importance()\n","\n","        del x_tr, x_val, y_tr, y_val\n","        gc.collect()\n","\n","    print(f\"\\nMean AUC = {score}\") # 폴드별 Validation 스코어 출력\n","    print(f\"OOF AUC = {roc_auc_score(y, y_oof)}\") # Out Of Fold Validation 스코어 출력\n","\n","    # 폴드별 피처 중요도 평균값 계산해서 저장\n","    fi_cols = [col for col in fi.columns if 'fold_' in col]\n","    fi['importance'] = fi[fi_cols].mean(axis=1)\n","\n","    return y_oof, test_preds, fi"],"metadata":{"id":"2HZP9C6v2vu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lgbm params\n","\n","model_params = {\n","    'objective': 'binary', # 이진 분류\n","    'boosting_type': 'gbdt',\n","    'metric': 'auc', # 평가 지표 설정\n","    'feature_fraction': 0.8, # 피처 샘플링 비율\n","    'bagging_fraction': 0.8, # 데이터 샘플링 비율\n","    'bagging_freq': 1,\n","    'n_estimators': 10000, # 트리 개수\n","    'early_stopping_rounds': 100,\n","    'seed': SEED,\n","    'verbose': -1,\n","    'n_jobs': -1,\n","}"],"metadata":{"id":"KV-LSDlydUPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test, y, features = feature_engineering1(data, '2011-12')\n","y_oof, test_preds_lgbm, fi = make_lgb_oof_prediction(train, y, test, features, model_params=model_params)"],"metadata":{"id":"o0kSJ4y79zXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.distplot(test_preds_lgbm)\n","plt.show()"],"metadata":{"id":"RBW7Q8ryatfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi = plot_feature_importances(fi)"],"metadata":{"id":"OLb-eOYaattv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최적의 파라미터 찾기 optuna\n","\n","'''\n","    하이퍼파라미터 탐색 범위를 정의하고 튜닝 및 검증을 진행하는 함수\n","'''\n","\n","def objective(trial, label=label_2011_11):\n","    lgb_params = {\n","        'objective': 'binary', # 이진 분류\n","        'boosting_type': 'gbdt',\n","        'num_leaves': trial.suggest_int('num_leaves', 2, 256), # num_leaves 값을 2-256까지 정수값 중에 사용\n","        'max_bin': trial.suggest_int('max_bin', 128, 256), # max_bin 값을 128-256까지 정수값 중에 사용\n","        # min_data_in_leaf 값을 10-40까지 정수값 중에 사용\n","        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 40),\n","        # 피처 샘플링 비율을 0.4-1.0까지 중에 uniform 분포로 사용\n","        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n","        # 데이터 샘플링 비율을 0.4-1.0까지 중에 uniform 분포로 사용\n","        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n","        # 데이터 샘플링 횟수를 1-7까지 정수값 중에 사용\n","        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n","        'n_estimators': 10000, # 트리 개수\n","        'early_stopping_rounds': 100,\n","        # L1 값을 1e-8-10.0까지 로그 uniform 분포로 사용\n","        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n","        # L2 값을 1e-8-10.0까지 로그 uniform 분포로 사용\n","        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n","        'seed': SEED,\n","        'verbose': -1,\n","        'n_jobs': -1,\n","    }\n","\n","    # oof prediction 함수 호출해서 out of fold validation 예측값을 얻어옴\n","    y_oof, test_preds, fi = make_lgb_oof_prediction(train, y, test, features, model_params=lgb_params)\n","\n","    # Validation 스코어 계산\n","    val_auc = roc_auc_score(label, y_oof)\n","\n","    return val_auc"],"metadata":{"id":"e83tQU5VEQiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# auc 최대화하는 방향으로 파라미터 탐색\n","\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=10) # 10회 동안 하이퍼 파라미터 탐색"],"metadata":{"id":"3FfYzYl0EJLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# validation 스코어 가장 잘나온\n","\n","study.best_params"],"metadata":{"id":"zKpcF3sLENI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 성능 가장 잘나온\n","\n","study.best_value"],"metadata":{"id":"l0shm8BuFa3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 하이퍼 파라미터 탐색\n","\n","study.trials_dataframe()"],"metadata":{"id":"SrNjwz5tFcU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파라미터 중요도\n","\n","optuna.visualization.matplotlib.plot_param_importances(study)\n","plt.show()"],"metadata":{"id":"857IMD3hFd6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파라미터 탐색 히스토리\n","\n","optuna.visualization.matplotlib.plot_optimization_history(study)\n","plt.show()"],"metadata":{"id":"4uJZTJ9DFf_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파라미터별 스코어 관계\n","\n","optuna.visualization.matplotlib.plot_slice(study)\n","plt.show()"],"metadata":{"id":"UafQkJ_yF2-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파라미터 카운투어\n","\n","optuna.visualization.matplotlib.plot_contour(study, params=['num_leaves','min_data_in_leaf'])\n","plt.show()"],"metadata":{"id":"qHYmOZSKGGJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.matplotlib.plot_parallel_coordinate(study, params=['num_leaves','min_data_in_leaf'])\n","plt.show()"],"metadata":{"id":"x-ynRN6yGM9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# xgboost K fold cross validation\n","\n","\n","'''\n","    학습 데이터(x_tr), 검증 데이터(x_val), 테스트 데이터(test)로 XGBoost 모델을\n","    학습, 교차(cross) 검증 및 테스트하고 사용된 피처들의 중요도를 반환하는 함수\n","'''\n","\n","def make_xgb_oof_prediction(train, y, test, features, model_params=None, folds=10):\n","    x_train = train[features]\n","    x_test = test[features]\n","\n","    # 테스트 데이터 예측값을 저장할 변수\n","    test_preds = np.zeros(x_test.shape[0])\n","\n","    # Out Of Fold Validation 예측 데이터를 저장할 변수\n","    y_oof = np.zeros(x_train.shape[0])\n","\n","    # 폴드별 평균 Validation 스코어를 저장할 변수\n","    score = 0\n","\n","    # 피처 중요도를 저장할 데이터 프레임 선언\n","    fi = pd.DataFrame()\n","    fi['feature'] = features\n","\n","    # Stratified K Fold 선언\n","    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n","\n","    for fold, (tr_idx, val_idx) in enumerate(skf.split(x_train, y)):\n","        # train index, validation index로 train 데이터를 나눔\n","        x_tr, x_val = x_train.loc[tr_idx, features], x_train.loc[val_idx, features]\n","        y_tr, y_val = y[tr_idx], y[val_idx]\n","\n","        print(f'fold: {fold+1}, x_tr.shape: {x_tr.shape}, x_val.shape: {x_val.shape}')\n","\n","        # XGBoost 데이터셋 선언\n","        dtrain = xgb.DMatrix(x_tr, label=y_tr)\n","        dvalid = xgb.DMatrix(x_val, label=y_val)\n","\n","        # XGBoost 모델 훈련\n","        clf = xgb.train(\n","            model_params,\n","            dtrain,\n","            num_boost_round=10000, # 트리 개수\n","            evals=[(dtrain, 'train'), (dvalid, 'valid')],  # Validation 성능을 측정할 수 있도록 설정\n","            verbose_eval=200,\n","            early_stopping_rounds=100\n","        )\n","\n","        # Validation 데이터 예측\n","        val_preds = clf.predict(dvalid)\n","\n","        # Validation index에 예측값 저장\n","        y_oof[val_idx] = val_preds\n","\n","        # 폴드별 Validation 스코어 출력\n","        print(f\"Fold {fold + 1} | AUC: {roc_auc_score(y_val, val_preds)}\")\n","        print('-'*80)\n","\n","        # score 변수에 폴드별 평균 Validation 스코어 저장\n","        score += roc_auc_score(y_val, val_preds) / folds\n","\n","        # 테스트 데이터 예측하고 평균해서 저장\n","        test_preds += clf.predict(xgb.DMatrix(x_test)) / folds\n","\n","        # 폴드별 피처 중요도 저장\n","        fi_tmp = pd.DataFrame.from_records([clf.get_score()]).T.reset_index()\n","        fi_tmp.columns = ['feature',f'fold_{fold+1}']\n","        fi = pd.merge(fi, fi_tmp, on='feature')\n","\n","        del x_tr, x_val, y_tr, y_val\n","        gc.collect()\n","\n","    print(f\"\\nMean AUC = {score}\") # 폴드별 평균 Validation 스코어 출력\n","    print(f\"OOF AUC = {roc_auc_score(y, y_oof)}\") # Out Of Fold Validation 스코어 출력\n","\n","    # 폴드별 피처 중요도 평균값 계산해서 저장\n","    fi_cols = [col for col in fi.columns if 'fold_' in col]\n","    fi['importance'] = fi[fi_cols].mean(axis=1)\n","\n","    return y_oof, test_preds, fi"],"metadata":{"id":"LFmpTpoXPd2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb_params = {\n","    'objective': 'binary:logistic', # 이진 분류\n","    'learning_rate': 0.1, # 학습률\n","    'max_depth': 6, # 트리 최고 깊이\n","    'colsample_bytree': 0.8, # 피처 샘플링 비율\n","    'subsample': 0.8, # 데이터 샘플링 비율\n","    'eval_metric': 'auc', # 평가 지표 설정\n","    'seed': SEED,\n","}"],"metadata":{"id":"Jn_vf63pavEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_oof, test_preds, fi_xgb = make_xgb_oof_prediction(train, y, test, features, model_params=xgb_params)"],"metadata":{"id":"AOOxIcGKawzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.distplot(test_preds)\n","plt.show()"],"metadata":{"id":"8DQGY4WSax0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_xgb = plot_feature_importances(fi_xgb)"],"metadata":{"id":"0HSfbcveeTdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# catboost K fold cross validation\n","\n","'''\n","    학습 데이터(x_tr), 검증 데이터(x_val), 테스트 데이터(test)로 CatBoost 모델을\n","    학습, 교차(cross) 검증 및 테스트하고 사용된 피처들의 중요도를 반환하는 함수\n","'''\n","\n","def make_cat_oof_prediction(train, y, test, features, categorical_features=None, model_params=None, folds=10):\n","    x_train = train[features]\n","    x_test = test[features]\n","\n","    # 테스트 데이터 예측값을 저장할 변수\n","    test_preds = np.zeros(x_test.shape[0])\n","\n","    # Out Of Fold Validation 예측 데이터를 저장할 변수\n","    y_oof = np.zeros(x_train.shape[0])\n","\n","    # 폴드별 평균 Validation 스코어를 저장할 변수\n","    score = 0\n","\n","    # 피처 중요도를 저장할 데이터 프레임 선언\n","    fi = pd.DataFrame()\n","    fi['feature'] = features\n","\n","    # Stratified K Fold 선언\n","    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n","\n","    for fold, (tr_idx, val_idx) in enumerate(skf.split(x_train, y)):\n","        # train index, validation index로 train 데이터를 나눔\n","        x_tr, x_val = x_train.loc[tr_idx, features], x_train.loc[val_idx, features]\n","        y_tr, y_val = y[tr_idx], y[val_idx]\n","\n","        print(f'fold: {fold+1}, x_tr.shape: {x_tr.shape}, x_val.shape: {x_val.shape}')\n","\n","        # CatBoost 모델 훈련\n","        clf = CatBoostClassifier(**model_params)\n","        clf.fit(x_tr, y_tr,\n","                eval_set=(x_val, y_val), # Validation 성능을 측정할 수 있도록 설정\n","                cat_features=categorical_features,\n","                use_best_model=True,\n","                verbose=True)\n","\n","        # Validation 데이터 예측\n","        val_preds = clf.predict_proba(x_val)[:,1]\n","\n","        # Validation index에 예측값 저장\n","        y_oof[val_idx] = val_preds\n","\n","        # 폴드별 Validation 스코어 출력\n","        print(f\"Fold {fold + 1} | AUC: {roc_auc_score(y_val, val_preds)}\")\n","        print('-'*80)\n","\n","        # score 변수에 폴드별 평균 Validation 스코어 저장\n","        score += roc_auc_score(y_val, val_preds) / folds\n","\n","        # 테스트 데이터 예측하고 평균해서 저장\n","        test_preds += clf.predict_proba(x_test)[:,1] / folds\n","\n","        # 폴드별 피처 중요도 저장\n","        fi[f'fold_{fold+1}'] = clf.feature_importances_\n","\n","        del x_tr, x_val, y_tr, y_val\n","        gc.collect()\n","\n","    print(f\"\\nMean AUC = {score}\") # 폴드별 평균 Validation 스코어 출력\n","    print(f\"OOF AUC = {roc_auc_score(y, y_oof)}\") # Out Of Fold Validation 스코어 출력\n","\n","    # 폴드별 피처 중요도 평균값 계산해서 저장\n","    fi_cols = [col for col in fi.columns if 'fold_' in col]\n","    fi['importance'] = fi[fi_cols].mean(axis=1)\n","\n","    return y_oof, test_preds, fi"],"metadata":{"id":"Qr95tHNh1O2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# catboost params\n","\n","cat_params = {\n","    'n_estimators': 10000, # 트리 개수\n","    'learning_rate': 0.07, # 학습률\n","    'eval_metric': 'AUC', # 평가 지표 설정\n","    'loss_function': 'Logloss', # 손실 함수 설정\n","    'random_seed': SEED,\n","    'metric_period': 100,\n","    'od_wait': 100, # early stopping round\n","    'depth': 6, # 트리 최고 깊이\n","    'rsm': 0.8, # 피처 샘플링 비율\n","}"],"metadata":{"id":"oYincX7tebr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_oof, test_preds, fi_cat = make_cat_oof_prediction(train, y, test, features, model_params=cat_params)"],"metadata":{"id":"8GbOo7c3babc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.distplot(test_preds)\n","plt.show()"],"metadata":{"id":"nOByfQc3efTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_cat = plot_feature_importances(fi_cat)"],"metadata":{"id":"_Jp2AXofek26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_JhffGLLelu5"},"execution_count":null,"outputs":[]}]}